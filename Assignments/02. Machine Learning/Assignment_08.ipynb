{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e865af5",
   "metadata": {},
   "source": [
    "**1. What exactly is a feature? Give an example to illustrate your point.**\n",
    "\n",
    "**Ans:**  A feature is an individual measurable property or characteristic of an object or entity. In the context of machine learning, features represent attributes or variables that describe instances of data.\n",
    "\n",
    "For example, in a dataset of houses, features could include square footage, number of bedrooms, and location.\n",
    "\n",
    "**2. What are the various circumstances in which feature construction is required?**\n",
    "\n",
    "**Ans:** \n",
    "Feature construction is required in various situations, including:\n",
    "\n",
    "- When relevant information is spread across multiple features.\n",
    "- When the existing features don't capture the underlying patterns.\n",
    "- When new domain-specific features can improve model performance.\n",
    "- When transforming features into a more suitable representation.\n",
    "\n",
    "**3. Describe how nominal variables are encoded.**\n",
    "\n",
    "**Ans:** Nominal variables are categorical variables without a meaningful order. They are encoded using techniques like one-hot encoding. Each category becomes a binary feature, indicating its presence or absence. \n",
    "\n",
    "For example, if a nominal variable is \"Color\" with categories \"Red,\" \"Green,\" and \"Blue,\" it would be encoded as three binary features.\n",
    "\n",
    "**4. Describe how numeric features are converted to categorical features.**\n",
    "\n",
    "**Ans:** Numeric features can be converted to categorical features by binning or discretizing them. \n",
    "\n",
    "For example, ages can be binned into categories like \"Child,\" \"Teen,\" \"Adult,\" and \"Senior.\"\n",
    "\n",
    "**5. Describe the feature selection wrapper approach. State the advantages and disadvantages of this approach?**\n",
    "\n",
    "**Ans:**  The feature selection wrapper approach involves selecting subsets of features based on model performance. It iteratively evaluates models with different feature combinations and selects the best-performing set. Advantages include optimized feature sets. Disadvantages are high computational cost and potential overfitting due to exhaustive search.\n",
    "\n",
    "**6. When is a feature considered irrelevant? What can be said to quantify it?**\n",
    "\n",
    "**Ans:** A feature is considered irrelevant if it doesn't contribute meaningful information for prediction. Irrelevance can be quantified using statistical measures like correlation coefficient or mutual information, indicating weak or no relationship with the target variable.\n",
    "\n",
    "**7. When is a function considered redundant? What criteria are used to identify features that could be redundant?**\n",
    "\n",
    "**Ans:** A function is considered redundant if it provides similar information to another feature. Criteria for identifying redundancy include high correlation between features or their ability to produce similar predictions when used in a model.\n",
    "\n",
    "**8. What are the various distance measurements used to determine feature similarity?**\n",
    "\n",
    "**Ans:** Various distance measurements include Euclidean, Manhattan, Cosine, Jaccard, and more. These metrics determine how similar or dissimilar features are.\n",
    "\n",
    "**9. State difference between Euclidean and Manhattan distances?**\n",
    "\n",
    "**Ans:** \n",
    "- **Euclidean Distance:** Calculates the shortest straight-line distance between two points in a Euclidean space.\n",
    "- **Manhattan Distance:** Calculates the distance between two points by summing the absolute differences of their coordinates along each dimension.\n",
    "\n",
    "**10. Distinguish between feature transformation and feature selection.**\n",
    "\n",
    "- **Feature Transformation:** Modifies or creates new features from existing ones, e.g., PCA, polynomial features.\n",
    "- **Feature Selection:** Chooses a subset of existing features, removing irrelevant or redundant ones.\n",
    "\n",
    "**11. Make brief notes on any two of the following:**\n",
    "**1.SVD (Standard Variable Diameter Diameter)**\n",
    "**2. Collection of features using a hybrid approach** \n",
    "\n",
    "Combining various feature extraction, transformation, and selection techniques to create a more comprehensive feature set.\n",
    "\n",
    "**3. The width of the silhouette**\n",
    "\n",
    "Silhouette width measures the separation between clusters in a clustering task.\n",
    "\n",
    "**4. Receiver operating characteristic curve**** \n",
    "\n",
    "A graphical plot that illustrates a classification model's performance by showing the trade-off between true positive rate (sensitivity) and false positive rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
